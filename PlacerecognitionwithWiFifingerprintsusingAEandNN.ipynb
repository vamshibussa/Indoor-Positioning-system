{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders and Neural Network for Place recognition with WiFi fingerprints\n",
    "Implementation of algorithm discussed in <a href=\"https://arxiv.org/pdf/1611.02049v1.pdf\">Low-effort place recognition with WiFi fingerprints using Deep Learning </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"trainingData.csv\",header = 0)\n",
    "\n",
    "features = np.asarray(dataset.iloc[:,0:520])\n",
    "features[features == 100] = -110\n",
    "features = (features - features.mean()) / features.var()\n",
    "\n",
    "labels = np.asarray(dataset[\"BUILDINGID\"].map(str) + dataset[\"FLOOR\"].map(str))\n",
    "labels = np.asarray(pd.get_dummies(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing UJIndoorLoc training data set into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = np.random.rand(len(features)) < 0.70\n",
    "train_x = features[train_val_split]\n",
    "train_y = labels[train_val_split]\n",
    "val_x = features[~train_val_split]\n",
    "val_y = labels[~train_val_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using UJIndoorLoc validation data set as testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(\"validationData.csv\",header = 0)\n",
    "\n",
    "test_features = np.asarray(test_dataset.iloc[:,0:520])\n",
    "test_features[test_features == 100] = -110\n",
    "test_features = (test_features - test_features.mean()) / test_features.var()\n",
    "\n",
    "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
    "test_labels = np.asarray(pd.get_dummies(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 520 \n",
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 128 \n",
    "n_hidden_3 = 64 \n",
    "\n",
    "n_classes = labels.shape[1]\n",
    "\n",
    "learning_rate = 0.00001\n",
    "training_epochs = 30\n",
    "batch_size = 15\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,n_input])\n",
    "Y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "\n",
    "# --------------------- Encoder Variables --------------- #\n",
    "\n",
    "e_weights_h1 = weight_variable([n_input, n_hidden_1])\n",
    "e_biases_h1 = bias_variable([n_hidden_1])\n",
    "\n",
    "e_weights_h2 = weight_variable([n_hidden_1, n_hidden_2])\n",
    "e_biases_h2 = bias_variable([n_hidden_2])\n",
    "\n",
    "e_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
    "e_biases_h3 = bias_variable([n_hidden_3])\n",
    "\n",
    "# --------------------- Decoder Variables --------------- #\n",
    "\n",
    "d_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
    "d_biases_h1 = bias_variable([n_hidden_2])\n",
    "\n",
    "d_weights_h2 = weight_variable([n_hidden_2, n_hidden_1])\n",
    "d_biases_h2 = bias_variable([n_hidden_1])\n",
    "\n",
    "d_weights_h3 = weight_variable([n_hidden_1, n_input])\n",
    "d_biases_h3 = bias_variable([n_input])\n",
    "\n",
    "# --------------------- DNN Variables ------------------ #\n",
    "\n",
    "dnn_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
    "dnn_biases_h1 = bias_variable([n_hidden_2])\n",
    "\n",
    "dnn_weights_h2 = weight_variable([n_hidden_2, n_hidden_2])\n",
    "dnn_biases_h2 = bias_variable([n_hidden_2])\n",
    "\n",
    "dnn_weights_out = weight_variable([n_hidden_2, n_classes])\n",
    "dnn_biases_out = bias_variable([n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,e_weights_h1),e_biases_h1))\n",
    "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,e_weights_h2),e_biases_h2))\n",
    "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,e_weights_h3),e_biases_h3))\n",
    "    return l3\n",
    "    \n",
    "def decode(x):\n",
    "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,d_weights_h1),d_biases_h1))\n",
    "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,d_weights_h2),d_biases_h2))\n",
    "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,d_weights_h3),d_biases_h3))\n",
    "    return l3\n",
    "\n",
    "def dnn(x):\n",
    "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,dnn_weights_h1),dnn_biases_h1))\n",
    "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,dnn_weights_h2),dnn_biases_h2))\n",
    "    out = tf.nn.softmax(tf.add(tf.matmul(l2,dnn_weights_out),dnn_biases_out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encode(X)\n",
    "decoded = decode(encoded) \n",
    "y_ = dnn(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cost_function = tf.reduce_mean(tf.pow(X - decoded, 2))\n",
    "s_cost_function = -tf.reduce_sum(Y * tf.log(y_))\n",
    "us_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(us_cost_function)\n",
    "s_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(s_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture\n",
    "Image take from: https://arxiv.org/pdf/1611.02049v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"AE.png\">\n",
    "<img src=\"NN.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  0.0660458436204\n",
      "Epoch:  1  Loss:  0.043526367739\n",
      "Epoch:  2  Loss:  0.0332489738804\n",
      "Epoch:  3  Loss:  0.0268834105527\n",
      "Epoch:  4  Loss:  0.022691431187\n",
      "Epoch:  5  Loss:  0.0197905028352\n",
      "Epoch:  6  Loss:  0.0175606289838\n",
      "Epoch:  7  Loss:  0.0158100251384\n",
      "Epoch:  8  Loss:  0.0144632592952\n",
      "Epoch:  9  Loss:  0.013416018006\n",
      "Epoch:  10  Loss:  0.0125764384511\n",
      "Epoch:  11  Loss:  0.011880635075\n",
      "Epoch:  12  Loss:  0.0112872774552\n",
      "Epoch:  13  Loss:  0.0107703027405\n",
      "Epoch:  14  Loss:  0.0103130813422\n",
      "Epoch:  15  Loss:  0.0099057022641\n",
      "Epoch:  16  Loss:  0.00954241765905\n",
      "Epoch:  17  Loss:  0.00921879420751\n",
      "Epoch:  18  Loss:  0.00893003553904\n",
      "Epoch:  19  Loss:  0.00867097473293\n",
      "Epoch:  20  Loss:  0.00843680622841\n",
      "Epoch:  21  Loss:  0.00822361831203\n",
      "Epoch:  22  Loss:  0.00802851717294\n",
      "Epoch:  23  Loss:  0.00784948391239\n",
      "Epoch:  24  Loss:  0.00768509676431\n",
      "Epoch:  25  Loss:  0.00753422384525\n",
      "Epoch:  26  Loss:  0.00739577292992\n",
      "Epoch:  27  Loss:  0.0072685709476\n",
      "Epoch:  28  Loss:  0.00715136928624\n",
      "Epoch:  29  Loss:  0.00704291936229\n",
      "Unsupervised pre-training finished...\n",
      "Epoch:  0  Loss:  35.4644009811  Training Accuracy:  0.441284 Validation Accuracy: 0.452573\n",
      "Epoch:  1  Loss:  28.7766546814  Training Accuracy:  0.672504 Validation Accuracy: 0.675412\n",
      "Epoch:  2  Loss:  23.2501297703  Training Accuracy:  0.746766 Validation Accuracy: 0.749916\n",
      "Epoch:  3  Loss:  19.1548787094  Training Accuracy:  0.799085 Validation Accuracy: 0.793138\n",
      "Epoch:  4  Loss:  16.0109953205  Training Accuracy:  0.826817 Validation Accuracy: 0.822402\n",
      "Epoch:  5  Loss:  13.4871686881  Training Accuracy:  0.856551 Validation Accuracy: 0.852001\n",
      "Epoch:  6  Loss:  11.4320143515  Training Accuracy:  0.89529 Validation Accuracy: 0.888665\n",
      "Epoch:  7  Loss:  9.7457325942  Training Accuracy:  0.914731 Validation Accuracy: 0.911537\n",
      "Epoch:  8  Loss:  8.34870194033  Training Accuracy:  0.929312 Validation Accuracy: 0.923646\n",
      "Epoch:  9  Loss:  7.17957329993  Training Accuracy:  0.940033 Validation Accuracy: 0.936092\n",
      "Epoch:  10  Loss:  6.19217909111  Training Accuracy:  0.955972 Validation Accuracy: 0.948705\n",
      "Epoch:  11  Loss:  5.35213388445  Training Accuracy:  0.965835 Validation Accuracy: 0.957787\n",
      "Epoch:  12  Loss:  4.63419392654  Training Accuracy:  0.971768 Validation Accuracy: 0.964346\n",
      "Epoch:  13  Loss:  4.01940045496  Training Accuracy:  0.975199 Validation Accuracy: 0.969055\n",
      "Epoch:  14  Loss:  3.49271189316  Training Accuracy:  0.9777 Validation Accuracy: 0.971746\n",
      "Epoch:  15  Loss:  3.04159371373  Training Accuracy:  0.979058 Validation Accuracy: 0.973259\n",
      "Epoch:  16  Loss:  2.65532740481  Training Accuracy:  0.980416 Validation Accuracy: 0.974941\n",
      "Epoch:  17  Loss:  2.32467572979  Training Accuracy:  0.980988 Validation Accuracy: 0.976455\n",
      "Epoch:  18  Loss:  2.04168820938  Training Accuracy:  0.982275 Validation Accuracy: 0.9778\n",
      "Epoch:  19  Loss:  1.79954934656  Training Accuracy:  0.984991 Validation Accuracy: 0.980323\n",
      "Epoch:  20  Loss:  1.59241599291  Training Accuracy:  0.986063 Validation Accuracy: 0.982678\n",
      "Epoch:  21  Loss:  1.41526155722  Training Accuracy:  0.986777 Validation Accuracy: 0.984023\n",
      "Epoch:  22  Loss:  1.26373681472  Training Accuracy:  0.987635 Validation Accuracy: 0.985032\n",
      "Epoch:  23  Loss:  1.13406948597  Training Accuracy:  0.98885 Validation Accuracy: 0.986546\n",
      "Epoch:  24  Loss:  1.02298575888  Training Accuracy:  0.989493 Validation Accuracy: 0.987555\n",
      "Epoch:  25  Loss:  0.927646996757  Training Accuracy:  0.99078 Validation Accuracy: 0.987891\n",
      "Epoch:  26  Loss:  0.845597426635  Training Accuracy:  0.990994 Validation Accuracy: 0.988228\n",
      "Epoch:  27  Loss:  0.774725965276  Training Accuracy:  0.991566 Validation Accuracy: 0.988732\n",
      "Epoch:  28  Loss:  0.713234658845  Training Accuracy:  0.992281 Validation Accuracy: 0.9889\n",
      "Epoch:  29  Loss:  0.659609173253  Training Accuracy:  0.992996 Validation Accuracy: 0.989909\n",
      "Supervised training finished...\n",
      "\n",
      "Testing Accuracy: 0.927093\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # ------------ 1. Training Autoencoders - Unsupervised Learning ----------- #\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_costs = np.empty(0)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([us_optimizer, us_cost_function],feed_dict={X: batch_x})\n",
    "            epoch_costs = np.append(epoch_costs,c)\n",
    "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs))\n",
    "    print(\"Unsupervised pre-training finished...\")\n",
    "    \n",
    "    \n",
    "    # ---------------- 2. Training NN - Supervised Learning ------------------ #\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_costs = np.empty(0)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
    "            epoch_costs = np.append(epoch_costs,c)\n",
    "        print(\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Training Accuracy: \", \\\n",
    "            session.run(accuracy, feed_dict={X: train_x, Y: train_y}), \\\n",
    "            \"Validation Accuracy:\", session.run(accuracy, feed_dict={X: val_x, Y: val_y}))\n",
    "            \n",
    "    print(\"Supervised training finished...\")\n",
    "    \n",
    "\n",
    "    print(\"\\nTesting Accuracy:\", session.run(accuracy, feed_dict={X: test_features, Y: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
